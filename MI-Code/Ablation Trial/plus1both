import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm
from mpl_toolkits.mplot3d import Axes3D
import time
import logging
import os
from datetime import datetime

# Configure logging with both file and console output
def setup_logging():
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f'PINNplus1both_training_{timestamp}.log' # Changed filename
    logger = logging.getLogger()
    if logger.hasHandlers():
        logger.handlers.clear()
    logging.basicConfig(
        level=logging.INFO,
        format='%(message)s',
        handlers=[
            logging.FileHandler(log_filename, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    return log_filename
log_filename = setup_logging()
logger = logging.getLogger(__name__)

torch.manual_seed(42)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger.info(f"Using device: {DEVICE}")


#------------------------------------------------------------------------------
# Network Structure (MLP)
# The network accepts layer and hidden dimension sizes as parameters
#------------------------------------------------------------------------------
class PINN_BS(nn.Module):
    def __init__(self, layers=4, hidden_dim=64):
        super(PINN_BS, self).__init__()
        layer_list = [nn.Linear(2, hidden_dim), nn.Tanh()]
        for _ in range(layers - 2):
            layer_list.extend([nn.Linear(hidden_dim, hidden_dim), nn.Tanh()])
        layer_list.append(nn.Linear(hidden_dim, 1))
        self.net = nn.Sequential(*layer_list)

    def forward(self, x):
        return self.net(x)

#------------------------------------------------------------------------------
# PDE loss
#------------------------------------------------------------------------------
def compute_pde_residual(model, t, S, K, r, sigma):
    V = model(torch.cat([t, S], dim=1))
    V_t = torch.autograd.grad(V, t, grad_outputs=torch.ones_like(V), create_graph=True)[0]
    V_s = torch.autograd.grad(V, S, grad_outputs=torch.ones_like(V), create_graph=True)[0]
    V_ss = torch.autograd.grad(V_s, S, grad_outputs=torch.ones_like(V_s), create_graph=True)[0]
    residual = V_t + r * S * V_s + 0.5 * sigma**2 * S**2 * V_ss - r * V
    return residual

def train_model(params):
    """Train a standard PINN model with mini-batch processing"""
    model = PINN_BS(layers=params['layers'], hidden_dim=params['hidden_dim']).to(DEVICE)
    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])
    
    scheduler = None
    if params['use_scheduler']:
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2000, gamma=0.7)

    loss_history = []
    start_time = time.time()

    logger.info("\n-- start training PINN --")
    logger.info("Epoch: Loss, Time(s)")
    
    for epoch in range(params['epochs']):
        # Dynamic sampling each epoch
        t_pde = torch.rand(params['N_interior'], 1, device=DEVICE) * params['T']
        S_pde = torch.rand(params['N_interior'], 1, device=DEVICE) * \
                (params['S_max'] - params['S_min']) + params['S_min']
        t_bc = torch.ones(params['N_bc'], 1, device=DEVICE) * params['T']
        S_bc = torch.rand(params['N_bc'], 1, device=DEVICE) * \
               (params['S_max'] - params['S_min']) + params['S_min']
        
        # Mini-batch iteration
        num_batches = max(1, params['N_interior'] // params['batch_size'])
        perm_interior = torch.randperm(params['N_interior'], device=DEVICE)
        perm_bc = torch.randperm(params['N_bc'], device=DEVICE)
        
        epoch_loss = 0.0
        for b in range(num_batches):
            start_idx = b * params['batch_size']
            end_idx = min(start_idx + params['batch_size'], params['N_interior'])
            batch_size_actual = end_idx - start_idx
            idx_interior = perm_interior[start_idx:end_idx]
            
            start_bc = (b * params['batch_size']) % params['N_bc']
            idx_bc = perm_bc[start_bc : start_bc + batch_size_actual]
            if len(idx_bc) < batch_size_actual:
                extra = batch_size_actual - len(idx_bc)
                idx_bc = torch.cat([idx_bc, perm_bc[:extra]])
            
            t_pde_batch = t_pde[idx_interior]
            S_pde_batch = S_pde[idx_interior]
            t_bc_batch = t_bc[idx_bc]
            S_bc_batch = S_bc[idx_bc]
            
            optimizer.zero_grad()
            
            # PDE loss
            t_pde_batch.requires_grad = True
            S_pde_batch.requires_grad = True
            pde_residual = compute_pde_residual(model, t_pde_batch, S_pde_batch, 
                                                params['K'], params['r'], params['sigma'])
            loss_pde = torch.mean(pde_residual**2)

            # Boundary condition loss
            V_pred_bc = model(torch.cat([t_bc_batch, S_bc_batch], dim=1))
            V_true_bc = torch.relu(S_bc_batch - params['K'])
            loss_bc = torch.mean((V_pred_bc - V_true_bc)**2)

            total_loss = loss_pde + params['lambda_bc'] * loss_bc

            # <<< MODIFICATION: MI-PINN part is removed >>>

            total_loss.backward()
            optimizer.step()
            epoch_loss += total_loss.item()
        
        epoch_loss /= num_batches
        loss_history.append(epoch_loss)
        
        if scheduler is not None:
            scheduler.step()

        if (epoch + 1) % 2000 == 0:
            elapsed = time.time() - start_time
            log_msg = f"Epoch [{epoch+1}/{params['epochs']}], Loss: {epoch_loss:.6f}, Time={elapsed:.2f}s"
            logger.info(log_msg)
    
    end_time = time.time()
    training_time = end_time - start_time
    logger.info(f"-- training finished, training time: {training_time:.2f} --")
    return model, loss_history, training_time


if __name__ == '__main__':
    n = 1  # For consistency (1D problem)

    shared_params = {
        'K': 1.0, 'T': 1.0, 'r': 0.05, 'sigma': 0.2,
        'S_min': 0.5, 'S_max': 1.5,
        'hidden_dim': 128, 'layers': 5, # PINN network parameters
        'lr': 1e-3, 'epochs': 10000, 'batch_size': 512,
        'use_scheduler': True,
        'lambda_bc': 100.0,
        # Proper scaling for 1D problem
        'N_interior': 10000 * n,
        'N_bc': 2000 * n,
    }
    
    logger.info("Starting PINN Training and Evaluation")
    logger.info(f"Parameters: {shared_params}")
    
    # <<< MODIFICATION: Only one model is trained now >>>
    pinn_model, loss_hist, training_time = train_model(shared_params)
    
    logger.info("\n-- Evaluation --")
    
    def black_scholes_call_numpy(t, S, K, T, r, sigma):
        epsilon = 1e-8
        T_t = T - t + epsilon
        d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T_t) / (sigma * np.sqrt(T_t))
        d2 = d1 - sigma * np.sqrt(T_t)
        return (S * norm.cdf(d1) - K * np.exp(-r * T_t) * norm.cdf(d2))
    
    T = shared_params['T']
    t_grid_np = np.linspace(0, T, 100)
    S_grid_np = np.linspace(shared_params['S_min'], shared_params['S_max'], 100)
    T_mesh, S_mesh = np.meshgrid(t_grid_np, S_grid_np)
    grid_points = torch.tensor(np.stack([T_mesh.flatten(), S_mesh.flatten()], axis=-1), dtype=torch.float32).to(DEVICE)
    
    V_true = black_scholes_call_numpy(T_mesh, S_mesh, **{k: shared_params[k] for k in ['K', 'T', 'r', 'sigma']})
    
    pinn_model.eval()
    with torch.no_grad():
        V_pred = pinn_model(grid_points).cpu().numpy().reshape(T_mesh.shape)
    
    # Calculate all three error metrics
    l2_error = np.linalg.norm(V_pred - V_true) / np.linalg.norm(V_true)
    mae = np.mean(np.abs(V_pred - V_true))
    mse = np.mean((V_pred - V_true)**2)
    
    log_msg_header = "\n" + "="*50
    log_msg_header += "\n" + " " * 18 + "PINN Results"
    log_msg_header += "\n" + "="*50
    logger.info(log_msg_header)
    
    log_msg_body = f"{'Training Time (s)':<25}: {training_time:<15.2f}"
    log_msg_body += f"\n{'L2 Relative Error':<25}: {l2_error:<15.4%}"
    log_msg_body += f"\n{'MAE (Mean Absolute Error)':<25}: {mae:<15.4e}"
    log_msg_body += f"\n{'MSE (Mean Squared Error)':<25}: {mse:<15.4e}"
    log_msg_body += "\n" + "="*50 + "\n"
    logger.info(log_msg_body)
    
    fig = plt.figure(figsize=(12, 10))
    
    # 1. loss curve
    ax1 = plt.subplot(2, 2, 1)
    ax1.plot(loss_hist, label='PINN Loss')
    ax1.set_yscale('log')
    ax1.set_title('Loss Convergence')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Total Loss (log scale)')
    ax1.legend(); ax1.grid(True, which="both", ls="--")
    
    # 2. final accuracy
    ax2 = plt.subplot(2, 2, 2)
    ax2.plot(S_grid_np, V_true[:, 0], 'k-', label='Analytical Solution', linewidth=2)
    ax2.plot(S_grid_np, V_pred[:, 0], 'r--', label=f'PINN (Err: {l2_error:.2%})')
    ax2.set_title('Price Curve at t=0')
    ax2.set_xlabel('Stock Price S')
    ax2.set_ylabel('Option Price V')
    ax2.legend(); ax2.grid(True)
    
    # 3. error surface
    ax3 = plt.subplot(2, 1, 2, projection='3d') # Changed layout for a single larger plot
    p1 = ax3.plot_surface(T_mesh, S_mesh, np.abs(V_pred - V_true), cmap='hot')
    ax3.set_title('PINN Absolute Error Surface')
    ax3.set_xlabel('Time t')
    ax3.set_ylabel('Stock Price S')
    ax3.set_zlabel('Absolute Error')
    fig.colorbar(p1, ax=ax3, fraction=0.046, pad=0.04)
    
    plt.tight_layout()
    
    plt.savefig('PINN_results.png', dpi=300) # Changed filename
    logger.info("Figure saved as: PINNplus1both_results.png")
    
    logger.info("Training and evaluation completed successfully!")
    if log_filename:
        logger.info(f"Log file saved as: {log_filename}")
        for handler in logging.getLogger().handlers:
            handler.flush()
    else:
        logger.warning("Log file could not be created.")
